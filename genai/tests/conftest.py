"""Pytest configuration and fixtures for GenAI service tests."""

import asyncio
from datetime import UTC, datetime
from typing import Any, Never
from unittest.mock import patch

import pytest
import pytest_asyncio
from fastapi.testclient import TestClient
from langchain.chat_models.base import BaseChatModel
from langchain_core.callbacks.manager import CallbackManagerForLLMRun
from langchain_core.messages import AIMessage, BaseMessage
from langchain_core.outputs import ChatGeneration, ChatResult
from pydantic import Field

from src.meilisearch import MeilisearchService
from src.models import (
    CommitAuthor,
    CommitContribution,
    CommitStats,
    CommitTree,
    ContributionType,
    GitHubUser,
    IssueContribution,
    PullRequestContribution,
    PullRequestRef,
    SummaryMetadata,
    SummaryResponse,
    generate_uuidv7,
)
from src.services import (
    ContributionsIngestionService,
    GitHubContentService,
    QuestionAnsweringService,
    SummaryService,
)

# Configure pytest-asyncio
pytest_plugins = ("pytest_asyncio",)


class MockChatModel(BaseChatModel):
    """Mock ChatModel that implements all required LangChain interfaces for testing."""

    temperature: float = Field(default=0.2)
    max_tokens: int | None = Field(default=None)

    class Config:
        """Pydantic configuration."""

        arbitrary_types_allowed = True

    def _generate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Generate a mock response."""
        # Create a simple mock response based on the last message
        last_message = messages[-1] if messages else None

        if last_message and "summary" in str(last_message.content).lower():
            content = "## Weekly Summary\n\nThis is a test summary generated by the mock LLM. The user made several contributions this week including commits and other activities."
        else:
            content = "This is a mock response from the test LLM."

        message = AIMessage(content=content)
        generation = ChatGeneration(message=message)
        return ChatResult(generations=[generation])

    async def _agenerate(
        self,
        messages: list[BaseMessage],
        stop: list[str] | None = None,
        run_manager: CallbackManagerForLLMRun | None = None,
        **kwargs: Any,
    ) -> ChatResult:
        """Async version of _generate."""
        return self._generate(messages, stop, run_manager, **kwargs)

    def bind_tools(self, tools: list[Any], **kwargs: Any) -> "MockChatModel":
        """Bind tools to the model (required for LangGraph agents)."""
        # Return a copy of self with tools bound
        bound_model = MockChatModel(temperature=self.temperature, max_tokens=self.max_tokens)
        bound_model._tools = tools
        return bound_model

    @property
    def _llm_type(self) -> str:
        """Return identifier of llm."""
        return "mock_chat_model"

    @property
    def _identifying_params(self) -> dict[str, Any]:
        """Get the identifying parameters."""
        return {"temperature": self.temperature, "max_tokens": self.max_tokens, "model_type": "mock"}


@pytest.fixture(scope="session", autouse=True)
def mock_llm_service():
    """Mock LLMService.create_llm to return a test LLM instead of making real API calls."""

    def create_mock_llm(**kwargs: Any) -> MockChatModel:
        """Create a mock LLM that behaves like a real LangChain LLM."""
        return MockChatModel(temperature=kwargs.get("temperature", 0.2), max_tokens=kwargs.get("max_tokens"))

    # Patch LLMService.create_llm to return our mock
    with patch("src.llm_service.LLMService.create_llm", side_effect=create_mock_llm):
        yield


@pytest_asyncio.fixture(scope="session", loop_scope="session")
async def meilisearch_service():
    """Create and initialize a real Meilisearch service for testing."""
    service = MeilisearchService()
    await service.initialize()
    yield service
    # Cleanup
    try:
        service.contributions_index.delete_all_documents()
    except Exception:
        pass  # Ignore cleanup errors


@pytest.fixture(scope="session")
def test_services(meilisearch_service):
    """Initialize test services with real Meilisearch."""
    ingestion_service = ContributionsIngestionService(meilisearch_service)
    github_content_service = GitHubContentService()
    qa_service = QuestionAnsweringService(github_content_service, meilisearch_service)
    summary_service = SummaryService(ingestion_service)

    # Link summary service to ingestion service (required for unified workflow)
    ingestion_service.summary_service = summary_service

    return {
        "meilisearch": meilisearch_service,
        "ingestion": ingestion_service,
        "github_content": github_content_service,
        "qa": qa_service,
        "summary": summary_service,
    }


@pytest.fixture(scope="session")
def test_client(test_services):
    """Create a test client with properly initialized services."""
    # Import app inside the fixture to avoid circular imports
    import app as app_module

    # Set up services in the app's service container
    app_module.services.meilisearch_service = test_services["meilisearch"]
    app_module.services.ingestion_service = test_services["ingestion"]
    app_module.services.qa_service = test_services["qa"]
    app_module.services.summary_service = test_services["summary"]

    test_client_instance = TestClient(app_module.app)
    yield test_client_instance

    # Cleanup
    app_module.services.meilisearch_service = None
    app_module.services.ingestion_service = None
    app_module.services.qa_service = None
    app_module.services.summary_service = None


@pytest_asyncio.fixture(loop_scope="function")
async def clean_services(test_services):
    """Clean service state between tests."""
    # Clear any stored data
    test_services["ingestion"].contributions_store.clear()
    test_services["ingestion"].embedding_jobs.clear()

    # Clear Meilisearch test data
    try:
        test_services["meilisearch"].contributions_index.delete_all_documents()
        await asyncio.sleep(0.1)  # Wait for deletion to complete
    except Exception:
        pass  # Ignore cleanup errors

    yield test_services


@pytest.fixture(scope="session")
def event_loop():
    """Create an instance of the default event loop for the test session."""
    loop = asyncio.get_event_loop_policy().new_event_loop()
    yield loop
    loop.close()


@pytest.fixture
def failing_github_service():
    """Mock GitHubContentService to always fail for testing error handling."""

    async def mock_fetch_contributions_fail(self, repository, user, week, metadata_list) -> Never:
        """Mock that always fails to test error handling."""
        msg = "Simulated GitHub API failure for testing"
        raise Exception(msg)

    with patch("src.contributions.GitHubContentService.fetch_contributions", mock_fetch_contributions_fail):
        yield


@pytest.fixture(autouse=True)
def auto_mock_github():
    """Automatically apply GitHub mocking to all tests."""

    def create_mock_commit(sha="abc123def456", repository="test/repo"):
        return CommitContribution(
            id=f"commit-{sha}",
            type=ContributionType.COMMIT,
            repository=repository,
            author="testuser",
            created_at=datetime.now(UTC),
            url=f"https://api.github.com/repos/{repository}/commits/{sha}",
            sha=sha,
            message="Fix authentication bug",
            tree=CommitTree(
                sha="tree123",
                url=f"https://api.github.com/repos/{repository}/git/trees/tree123",
            ),
            parents=[],
            author_info=CommitAuthor(
                name="Test User",
                email="testuser@example.com",
                date=datetime.now(UTC),
            ),
            committer=CommitAuthor(
                name="Test User",
                email="testuser@example.com",
                date=datetime.now(UTC),
            ),
            stats=CommitStats(total=15, additions=10, deletions=5),
            files=[],
        )

    def create_mock_pr(pr_id="42", repository="test/repo"):
        return PullRequestContribution(
            id=f"pr-{pr_id}",
            type=ContributionType.PULL_REQUEST,
            repository=repository,
            author="testuser",
            created_at=datetime.now(UTC),
            url=f"https://api.github.com/repos/{repository}/pulls/{pr_id}",
            number=int(pr_id),
            title="Add user management feature",
            body="This PR adds comprehensive user management functionality",
            state="open",
            locked=False,
            user=GitHubUser(login="testuser", id=12345, type="User"),
            head=PullRequestRef(
                label="testuser:feature-branch",
                ref="feature-branch",
                sha="def456ghi789",
                repo={"name": "repo", "full_name": repository},
            ),
            base=PullRequestRef(
                label="test:main",
                ref="main",
                sha="ghi789jkl012",
                repo={"name": "repo", "full_name": repository},
            ),
            merged=False,
            comments=0,
            review_comments=0,
            commits=1,
            additions=50,
            deletions=10,
            changed_files=3,
            comments_data=[],
            reviews_data=[],
            commits_data=[],
            files_data=[],
        )

    def create_mock_issue(issue_id="15", repository="test/repo"):
        return IssueContribution(
            id=f"issue-{issue_id}",
            type=ContributionType.ISSUE,
            repository=repository,
            author="testuser",
            created_at=datetime.now(UTC),
            url=f"https://api.github.com/repos/{repository}/issues/{issue_id}",
            number=int(issue_id),
            title="Performance optimization needed",
            body="The application is running slowly with large datasets",
            state="open",
            locked=False,
            user=GitHubUser(login="testuser", id=12345, type="User"),
            comments=0,
            comments_data=[],
            events_data=[],
        )

    async def mock_fetch_contributions(self, repository, user, week, metadata_list):
        """Mock fetch_contributions to return test data or simulate failures."""
        # Check if we should simulate a failure
        if hasattr(self, "_should_fail") and self._should_fail:
            msg = "Simulated GitHub API failure"
            raise Exception(msg)

        # Check for invalid repository to simulate 404
        if repository == "invalid/repo":
            msg = "Repository not found (404)"
            raise Exception(msg)

        results = []
        for metadata in metadata_list:
            # Handle both dict and object metadata
            if hasattr(metadata, "selected"):
                selected = metadata.selected
                contribution_type = metadata.type
                contribution_id = metadata.id
            else:
                selected = metadata.get("selected", True)
                contribution_type = metadata.get("type")
                contribution_id = metadata.get("id")

            if not selected:
                continue

            # Simulate individual contribution fetch failures
            if contribution_id == "fail123":
                continue  # Skip this one to simulate partial failure

            if contribution_type == "commit":
                results.append(create_mock_commit(contribution_id, repository))
            elif contribution_type == "pull_request":
                results.append(create_mock_pr(contribution_id, repository))
            elif contribution_type == "issue":
                results.append(create_mock_issue(contribution_id, repository))

        return results

    # Additional mock methods for agent tools
    async def mock_get_file_content(self, repository, file_path) -> str | None:
        if file_path == "notfound.py":
            return None
        return f"# Mock content of {file_path} in {repository}"

    async def mock_search_code(self, repository, query):
        return [
            {
                "path": "src/example.py",
                "html_url": f"https://github.com/{repository}/blob/main/src/example.py",
            }
        ]

    async def mock_search_issues_and_prs(self, repository, query, is_pr=False, is_open=None):
        if is_pr:
            return [
                {
                    "number": 1,
                    "title": "Mock PR",
                    "state": "open",
                    "html_url": f"https://github.com/{repository}/pull/1",
                }
            ]
        return [
            {
                "number": 2,
                "title": "Mock Issue",
                "state": "open",
                "html_url": f"https://github.com/{repository}/issues/2",
            }
        ]

    async def mock_search_commits(self, repository, query):
        return [
            {
                "sha": "abc123",
                "commit": {"message": "Mock commit"},
                "html_url": f"https://github.com/{repository}/commit/abc123",
            }
        ]

    async def mock_get_commit_details(self, repository, sha) -> None:
        return None

    async def mock_get_issue_details(self, repository, issue_number) -> None:
        return None

    async def mock_get_pull_request_details(self, repository, pr_number) -> None:
        return None

    # Apply all patches
    with (
        patch("src.contributions.GitHubContentService.fetch_contributions", mock_fetch_contributions),
        patch("src.contributions.GitHubContentService.get_file_content", mock_get_file_content),
        patch("src.contributions.GitHubContentService.search_code", mock_search_code),
        patch("src.contributions.GitHubContentService.search_issues_and_prs", mock_search_issues_and_prs),
        patch("src.contributions.GitHubContentService.search_commits", mock_search_commits),
        patch("src.contributions.GitHubContentService.get_commit_details", mock_get_commit_details),
        patch("src.contributions.GitHubContentService.get_issue_details", mock_get_issue_details),
        patch("src.contributions.GitHubContentService.get_pull_request_details", mock_get_pull_request_details),
    ):
        yield


@pytest.fixture
def mock_summary_for_api():
    """Automatically mock summary service to prevent OpenAI API calls during tests."""

    async def mock_generate_summary(self, user, week, summary_id=None):
        """Mock summary generation to return test data quickly."""
        import structlog

        logger = structlog.get_logger()
        logger.info("Mock generate_summary called", user=user, week=week, summary_id=summary_id)

        try:
            if summary_id is None:
                summary_id = generate_uuidv7()

            # Get actual contributions count from the ingestion service
            contributions = self.ingestion_service.get_user_week_contributions(user, week)
            total_contributions = len(contributions)

            # Count by type
            commits_count = sum(1 for c in contributions if getattr(c, "type", None) == ContributionType.COMMIT)
            prs_count = sum(1 for c in contributions if getattr(c, "type", None) == ContributionType.PULL_REQUEST)
            issues_count = sum(1 for c in contributions if getattr(c, "type", None) == ContributionType.ISSUE)
            releases_count = sum(1 for c in contributions if getattr(c, "type", None) == ContributionType.RELEASE)

            # Extract repositories
            repositories = list({getattr(c, "repository", "test/repo") for c in contributions})
            if not repositories:
                repositories = ["test/repo"]

            # Create mock metadata with actual data
            metadata = SummaryMetadata(
                total_contributions=total_contributions,
                commits_count=commits_count,
                pull_requests_count=prs_count,
                issues_count=issues_count,
                releases_count=releases_count,
                repositories=repositories,
                time_period=week,
                generated_at=datetime.now(UTC),
            )

            # Create mock summary response
            summary_response = SummaryResponse(
                summary_id=summary_id,
                user=user,
                week=week,
                overview=f"Test summary for {user} in week {week}",
                commits_summary="Made 1 commit this week",
                pull_requests_summary="No pull requests created this week",
                issues_summary="No issues created this week",
                releases_summary="No releases published this week",
                analysis="Test analysis of contributions",
                key_achievements=["Completed development tasks"],
                areas_for_improvement=["Continue consistent contribution patterns"],
                metadata=metadata,
                generated_at=datetime.now(UTC),
            )

            logger.info("Mock generate_summary returning", summary_id=summary_response.summary_id)
            return summary_response

        except Exception as e:
            logger.exception("Error in mock_generate_summary", error=str(e))
            raise

    # Patch the SummaryService.generate_summary method
    with patch.object(SummaryService, "generate_summary", mock_generate_summary):
        yield
